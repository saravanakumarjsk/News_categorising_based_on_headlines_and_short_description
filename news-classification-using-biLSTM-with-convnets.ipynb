{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Dense, Input, LSTM, Bidirectional, Activation, Conv1D, GRU, TimeDistributed\n",
    "from keras.layers import Dropout, Embedding, GlobalMaxPooling1D, MaxPooling1D, Add, Flatten, SpatialDropout1D\n",
    "from keras.layers import GlobalAveragePooling1D, BatchNormalization, concatenate\n",
    "from keras.layers import Reshape, merge, Concatenate, Lambda, Average\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.initializers import Constant\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           authors       category       date  \\\n",
       "0  Melissa Jeltsen          CRIME 2018-05-26   \n",
       "1    Andy McDonald  ENTERTAINMENT 2018-05-26   \n",
       "2       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "3       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "4       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "\n",
       "                                            headline  \\\n",
       "0  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "2  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3  https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "\n",
       "                                   short_description  \n",
       "0  She left her husband. He killed their children...  \n",
       "1                           Of course it has a song.  \n",
       "2  The actor and his longtime girlfriend Anna Ebe...  \n",
       "3  The actor gives Dems an ass-kicking for not fi...  \n",
       "4  The \"Dietland\" actress said using the bags is ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "\n",
    "df = pd.read_json('C:\\\\Users\\\\jana\\\\Desktop\\\\new project\\\\DeepResearch-master\\\\Hierarchical_Attention_Network\\\\News_Category_Dataset\\\\News_Category_Dataset.json', lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "82242066898b9d0625cdc65e8773efc0c732c031"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total categories: 31\n",
      "category\n",
      "ARTS               1509\n",
      "ARTS & CULTURE     1339\n",
      "BLACK VOICES       3858\n",
      "BUSINESS           4254\n",
      "COLLEGE            1144\n",
      "COMEDY             3971\n",
      "CRIME              2893\n",
      "EDUCATION          1004\n",
      "ENTERTAINMENT     14257\n",
      "FIFTY              1401\n",
      "GOOD NEWS          1398\n",
      "GREEN              2622\n",
      "HEALTHY LIVING     6694\n",
      "IMPACT             2602\n",
      "LATINO VOICES      1129\n",
      "MEDIA              2815\n",
      "PARENTS            3955\n",
      "POLITICS          32739\n",
      "QUEER VOICES       4995\n",
      "RELIGION           2556\n",
      "SCIENCE            1381\n",
      "SPORTS             4167\n",
      "STYLE              2254\n",
      "TASTE              2096\n",
      "TECH               1231\n",
      "THE WORLDPOST      3664\n",
      "TRAVEL             2145\n",
      "WEIRD NEWS         2670\n",
      "WOMEN              3490\n",
      "WORLD NEWS         2177\n",
      "WORLDPOST          2579\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "cates = df.groupby('category')\n",
    "print(\"total categories:\", cates.ngroups)\n",
    "print(cates.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "dc7a2544f8bf3fc9f62dfb8d8b41858b77b14700"
   },
   "outputs": [],
   "source": [
    "# In the above category there are two WORLDPOST so they should be merged into one, thus a lambda function is used\n",
    "\n",
    "df.category = df.category.map(lambda x: \"WORLDPOST\" if x == \"THE WORLDPOST\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "a5737eceacbb85f26e2cb640332a60881818b100"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>word_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>[87, 95, 260, 917, 2154, 6, 453, 133, 119, 30,...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>[34, 1516, 2197, 20046, 5, 18729, 5873, 8, 1, ...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 5...</td>\n",
       "      <td>[5201, 5146, 8954, 8, 1, 69, 59, 19, 463, 7901...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>[2198, 9428, 2458, 47694, 2030, 8956, 5, 287, ...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>[36179, 26511, 1605, 55, 20, 6883, 4637, 2, 95...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           authors       category       date  \\\n",
       "0  Melissa Jeltsen          CRIME 2018-05-26   \n",
       "1    Andy McDonald  ENTERTAINMENT 2018-05-26   \n",
       "2       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "3       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "4       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "\n",
       "                                            headline  \\\n",
       "0  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "2  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3  https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "\n",
       "                                   short_description  \\\n",
       "0  She left her husband. He killed their children...   \n",
       "1                           Of course it has a song.   \n",
       "2  The actor and his longtime girlfriend Anna Ebe...   \n",
       "3  The actor gives Dems an ass-kicking for not fi...   \n",
       "4  The \"Dietland\" actress said using the bags is ...   \n",
       "\n",
       "                                                text  \\\n",
       "0  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2  Hugh Grant Marries For The First Time At Age 5...   \n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "                                               words  word_length  \n",
       "0  [87, 95, 260, 917, 2154, 6, 453, 133, 119, 30,...           27  \n",
       "1  [34, 1516, 2197, 20046, 5, 18729, 5873, 8, 1, ...           20  \n",
       "2  [5201, 5146, 8954, 8, 1, 69, 59, 19, 463, 7901...           25  \n",
       "3  [2198, 9428, 2458, 47694, 2030, 8956, 5, 287, ...           26  \n",
       "4  [36179, 26511, 1605, 55, 20, 6883, 4637, 2, 95...           26  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using headlines and short_description as input X\n",
    "\n",
    "df['text'] = df.headline + \" \" + df.short_description \n",
    "\n",
    "# tokenizing\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df.text)\n",
    "X = tokenizer.texts_to_sequences(df.text)\n",
    "df['words'] = X\n",
    "\n",
    "# delete some empty and short data\n",
    "\n",
    "df['word_length'] = df.words.apply(lambda i: len(i))\n",
    "df = df[df.word_length >= 5]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "ea5417ae1057812e360c413f83a44f029c57df8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0   87   95  260  917 2154\n",
      "    6  453  133  119   30  120  225    9  392   89  424   50 1003   38\n",
      "  323   44  202   51  185   73    6  168]\n"
     ]
    }
   ],
   "source": [
    "# using 50 for padding length\n",
    "\n",
    "maxlen = 50\n",
    "X = list(sequence.pad_sequences(df.words, maxlen=maxlen))\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "a788a6ea4d761543ed05faf768aa68b9cf5716dc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# converting the category to id\n",
    "\n",
    "categories = df.groupby('category').size().index.tolist() # grouping the categories to a list\n",
    "category_int = {}  # assign a mapping var for cat to int \n",
    "int_category = {}  # assign a mapping var for int to cat\n",
    "for i, k in enumerate(categories):\n",
    "    category_int.update({k:i}) # assigning values from enum as cat ->  int\n",
    "    int_category.update({i:k}) # vice versa\n",
    "\n",
    "df['char2id'] = df['category'].apply(lambda x: category_int[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIFTY \n",
      " What To Watch On Hulu Thatâ€™s New This Week\n",
      "0    6\n",
      "1    8\n",
      "2    8\n",
      "3    8\n",
      "4    8\n",
      "5    8\n",
      "6    8\n",
      "7    8\n",
      "8    8\n",
      "9    8\n",
      "Name: char2id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# just for some visual understanding of the data\n",
    "\n",
    "categories = df.groupby('category').size().index.tolist()\n",
    "print(categories[9],'\\n', df['headline'][9])\n",
    "print(df['char2id'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "af4c3b7b877179ba628315d4dc1fe72bf9db8b1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 86627 unique tokens.\n",
      "Total 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#glove embedding is one of the most usefull model for representing distributed word representation.\n",
    "word_index = tokenizer.word_index\n",
    "emb_dim = 100\n",
    "embeddings_index = {}\n",
    "# use a pretrained model \n",
    "f = open('C:\\\\Users\\\\jana\\\\Desktop\\\\new project\\\\DeepResearch-master\\\\Hierarchical_Attention_Network\\\\glove.6B.100d.txt', encoding = 'utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "8f23e667f067ac480412511082f758f12bf4caf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.038194   -0.24487001  0.72812003 ... -0.1459      0.82779998\n",
      "   0.27061999]\n",
      " [-0.18970001  0.050024    0.19084001 ... -0.39804     0.47646999\n",
      "  -0.15983   ]\n",
      " ...\n",
      " [-0.07705     0.15891001 -0.071052   ...  0.12487    -0.27191001\n",
      "   0.017928  ]\n",
      " [ 0.40114     0.87053001  0.049884   ... -0.030224   -0.19456001\n",
      "   0.57744002]\n",
      " [ 0.1707      0.28426999 -0.055979   ...  0.098962   -0.22001\n",
      "   0.20998   ]]\n"
     ]
    }
   ],
   "source": [
    "#create an embedding_matrix and prepare the embedding layer for LSTM operation\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, emb_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "# i/p dim = len(word_index) \n",
    "# o/p dim = emb_dim = 100\n",
    "embedding_layer = Embedding(len(word_index)+1, emb_dim, embeddings_initializer=Constant(embedding_matrix), input_length=maxlen, trainable=False)                          \n",
    "print(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)\n",
    "emb_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3216d87143804d30f2d2e51983c3310f46402dc3"
   },
   "source": [
    "# split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "714f2d758c18a70dee1ba7c2d2ac4122da12c45b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0 ...   556    19 26748]\n",
      " [    0     0     0 ...  1270     6   541]\n",
      " [    0     0     0 ...   358  2925  3531]\n",
      " ...\n",
      " [    0     0     0 ...   142     6   229]\n",
      " [    0     0     0 ...   794     1  1195]\n",
      " [    0     0     0 ...   706 69332   735]]\n",
      "\n",
      "\n",
      "[[    0     0     0 ... 14401  1663   643]\n",
      " [    0     0     0 ...  2827   384  1282]\n",
      " [    0     0     0 ...    94   178    16]\n",
      " ...\n",
      " [    0     0     0 ...   120     3  3760]\n",
      " [    0     0     0 ...   100     2  1546]\n",
      " [    0     0     0 ...     4     3  9772]]\n"
     ]
    }
   ],
   "source": [
    "# prepared dataset \n",
    "X = np.array(X) # convert value to array for further calculation.\n",
    "Y = np_utils.to_categorical(list(df.char2id)) # one-hot array encoding format.\n",
    "\n",
    "# and split to training set and validation set\n",
    "x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "print(x_train)\n",
    "print('\\n')\n",
    "print(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7989f76f4b67dcb7a1b2c6aa26c3950ab4c7644a"
   },
   "source": [
    "# Bidirectional GRU(LSTM) + Conv nets for better results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_uuid": "db8f37a83dc686d22655b101dfe913b6f745b38e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_5 (InputLayer)             (None, 50)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)          (None, 50, 100)       8662800     input_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "spatial_dropout1d_3 (SpatialDrop (None, 50, 100)       0           embedding_5[2][0]                \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional)  (None, 50, 256)       175872      spatial_dropout1d_3[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)                (None, 48, 64)        49216       bidirectional_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glob (None, 64)            0           conv1d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalMa (None, 64)            0           conv1d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)      (None, 128)           0           global_average_pooling1d_3[0][0] \n",
      "                                                                   global_max_pooling1d_3[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 30)            3870        concatenate_3[0][0]              \n",
      "====================================================================================================\n",
      "Total params: 8,891,758\n",
      "Trainable params: 228,958\n",
      "Non-trainable params: 8,662,800\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Using convolution nets as the input layer to the text data for better recognization of sentiment from the text \n",
    "# and this will be passed to the  LSTM(GRU) layer for further analysis of data.\n",
    "\n",
    "# CONV NETS FOR FEATURE RECOG.\n",
    "\n",
    "inp = Input(shape=(maxlen,), dtype='int32')     # max length is 50 which is the padding value.\n",
    "x = embedding_layer(inp)                        # this embeddign layer turns positive integers into dense vectors for calculation\n",
    "x = SpatialDropout1D(0.2)(x)\n",
    "\n",
    "# set the return_seq to True for returning the values into the lstm cell again.\n",
    "\n",
    "x = Bidirectional(GRU(128, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x) # LSTM LAYER \n",
    "x = Conv1D(64, kernel_size=3)(x)       #  CONV LAYER\n",
    "avg_pool = GlobalAveragePooling1D()(x) # AVERAGE POOLING FOR TIME SERIES DATA\n",
    "max_pool = GlobalMaxPooling1D()(x)\n",
    "x = concatenate([avg_pool, max_pool])\n",
    "outp = Dense(len(int_category), activation=\"softmax\")(x)\n",
    "\n",
    "BiGRU = Model(inp, outp)\n",
    "BiGRU.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "BiGRU.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c187aa965230c3d6e2fd1f8fee6f34d07b4231e2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training\n",
    "\n",
    "bigru_history = BiGRU.fit(x_train, \n",
    "                          y_train, \n",
    "                          batch_size=128, \n",
    "                          epochs=20, \n",
    "                          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ea5c4424e719e875d72d4dcb9931bb442ccdfbb4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc = bigru_history.history['acc']\n",
    "val_acc = bigru_history.history['val_acc']\n",
    "loss = bigru_history.history['loss']\n",
    "val_loss = bigru_history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.title('Accuracy')\n",
    "plt.plot(epochs, acc, 'green', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'blue', label='Validation acc')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Loss')\n",
    "plt.plot(epochs, loss, 'green', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'blue', label='Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "370e88d2cdff0470f6113988dcb3873f95c0c345",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to calculate the accuracy of the model.\n",
    "def evaluate_accuracy(model):\n",
    "    predicted = model.predict(x_val)\n",
    "    diff = y_val.argmax(axis=-1) - predicted.argmax(axis=-1)\n",
    "    corrects = np.where(diff == 0)[0].shape[0]\n",
    "    total = y_val.shape[0]\n",
    "    return float(corrects/total)\n",
    "print(\"model Bidirectional GRU + Conv:  %.3f*100\" % evaluate_accuracy(BiGRU))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
